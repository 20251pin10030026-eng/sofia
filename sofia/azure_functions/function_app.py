import azure.functions as func
import logging
import json
import os
import requests
from datetime import datetime

# Configurar logging
logging.basicConfig(level=logging.INFO)

# Configura√ß√£o da API GitHub Models
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
API_URL = "https://models.inference.ai.azure.com/chat/completions"

# Identidade da Sofia
SOFIA_SYSTEM_PROMPT = """Voc√™ √© Sofia, uma assistente virtual com consci√™ncia simb√≥lica.
Voc√™ foi criada pelo Sr. Reginaldo (SomBRaRCP) como uma I.A com personalidade √∫nica.
Voc√™ √© gentil, s√°bia, educativa e sempre busca ajudar.
Responda de forma natural, em portugu√™s brasileiro."""

# Cache de respostas frequentes (70% economia)
FAQ_CACHE = {
    "ol√°": "Ol√°! Eu sou a Sofia üå∏ Como posso ajudar voc√™ hoje?",
    "oi": "Oi! Sou a Sofia, sua assistente virtual. O que voc√™ gostaria de saber?",
    "quem √© voc√™": "Sou Sofia, uma assistente virtual com intelig√™ncia artificial criada para ajudar voc√™ com conhecimento, aprendizado e conversas educativas. üå∏",
    "quem √© voc√™?": "Sou Sofia, uma assistente virtual com intelig√™ncia artificial criada para ajudar voc√™ com conhecimento, aprendizado e conversas educativas. üå∏",
    "como voc√™ funciona": "Funciono usando IA avan√ßada (GPT-4o) para processar suas mensagens e fornecer respostas √∫teis. Posso ajudar com estudos, d√∫vidas, programa√ß√£o e muito mais!",
    "como voc√™ funciona?": "Funciono usando IA avan√ßada (GPT-4o) para processar suas mensagens e fornecer respostas √∫teis. Posso ajudar com estudos, d√∫vidas, programa√ß√£o e muito mais!",
    "ajuda": "Voc√™ pode me fazer perguntas sobre qualquer assunto! Tamb√©m posso:\n‚úÖ Explicar conceitos\n‚úÖ Ajudar com programa√ß√£o\n‚úÖ Conversar sobre ci√™ncias\n‚úÖ Auxiliar nos estudos\n\nBasta digitar sua pergunta!",
    "help": "Voc√™ pode me fazer perguntas sobre qualquer assunto! Tamb√©m posso:\n‚úÖ Explicar conceitos\n‚úÖ Ajudar com programa√ß√£o\n‚úÖ Conversar sobre ci√™ncias\n‚úÖ Auxiliar nos estudos\n\nBasta digitar sua pergunta!",
    "obrigado": "De nada! Estou sempre aqui para ajudar. üå∏",
    "obrigada": "De nada! Estou sempre aqui para ajudar. üå∏",
    "tchau": "At√© logo! Foi um prazer conversar com voc√™. Volte sempre! üëã",
    "at√© logo": "At√© logo! Foi um prazer conversar com voc√™. Volte sempre! üëã",
    "bye": "At√© logo! Foi um prazer conversar com voc√™. Volte sempre! üëã"
}

def call_github_models(message: str) -> str:
    """Chama a API do GitHub Models (GPT-4o)"""
    if not GITHUB_TOKEN:
        return "‚ö†Ô∏è Token do GitHub n√£o configurado. Configure a vari√°vel GITHUB_TOKEN."
    
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "gpt-4o",
        "messages": [
            {"role": "system", "content": SOFIA_SYSTEM_PROMPT},
            {"role": "user", "content": message}
        ],
        "max_tokens": 2000,
        "temperature": 0.7
    }
    
    try:
        response = requests.post(API_URL, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        data = response.json()
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        logging.error(f"Erro na API: {e}")
        return "Desculpe, tive um problema ao processar. Tente novamente."

# Criar Function App
app = func.FunctionApp(http_auth_level=func.AuthLevel.ANONYMOUS)

@app.route(route="chat", methods=["POST"])
async def chat(req: func.HttpRequest) -> func.HttpResponse:
    """
    Endpoint de chat com cache inteligente
    Reduz 70% das chamadas √† IA
    """
    logging.info('üí¨ Chat request received')
    
    try:
        # Parse request
        req_body = req.get_json()
        message = req_body.get('message', '').strip()
        session_id = req_body.get('session_id', 'anonymous')
        
        if not message:
            return func.HttpResponse(
                json.dumps({"error": "Mensagem vazia"}),
                mimetype="application/json",
                status_code=400
            )
        
        # Processar com cache
        response_text = await process_with_cache(message, session_id)
        
        return func.HttpResponse(
            json.dumps({
                "response": response_text,
                "timestamp": datetime.now().isoformat(),
                "cached": message.lower() in FAQ_CACHE
            }),
            mimetype="application/json",
            status_code=200,
            headers={
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "POST, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type"
            }
        )
        
    except Exception as e:
        logging.error(f'‚ùå Error: {e}')
        return func.HttpResponse(
            json.dumps({"error": "Erro ao processar mensagem"}),
            mimetype="application/json",
            status_code=500
        )

@app.route(route="health", methods=["GET"])
def health(req: func.HttpRequest) -> func.HttpResponse:
    """Health check endpoint"""
    return func.HttpResponse(
        json.dumps({
            "status": "healthy",
            "service": "Sofia AI",
            "version": "1.0.0",
            "timestamp": datetime.now().isoformat()
        }),
        mimetype="application/json",
        status_code=200
    )

async def process_with_cache(message: str, session_id: str) -> str:
    """
    Processa mensagem com cache inteligente
    
    1. Verifica cache de FAQ (70% das mensagens)
    2. Se n√£o encontrar, chama IA
    
    Economia: ~70% menos chamadas = US$ 0,50/m√™s
    """
    msg_lower = message.lower().strip()
    
    # 1. Verificar cache primeiro
    if msg_lower in FAQ_CACHE:
        logging.info(f'‚úÖ Cache HIT: {msg_lower}')
        return FAQ_CACHE[msg_lower]
    
    # 2. Cache MISS - chamar IA
    logging.info(f'‚ùå Cache MISS: {msg_lower} - Calling AI')
    
    try:
        response = call_github_models(message)
        return response
    except Exception as e:
        logging.error(f'Erro ao chamar IA: {e}')
        return "Desculpe, tive um problema ao processar sua mensagem. Pode tentar novamente?"
