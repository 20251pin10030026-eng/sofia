"""
M√≥dulo de Busca Web para Sofia
Permite buscar informa√ß√µes na internet e acessar links fornecidos
"""

import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from typing import Optional, Dict, List, Any
import os

def _is_url(texto: str) -> bool:
    """Verifica se o texto cont√©m uma URL v√°lida"""
    url_pattern = re.compile(
        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    )
    return bool(url_pattern.search(texto))


def _extrair_urls(texto: str) -> List[str]:
    """Extrai todas as URLs de um texto"""
    url_pattern = re.compile(
        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    )
    return url_pattern.findall(texto)


def acessar_link(url: str, timeout: int = 10) -> Optional[Dict[str, Any]]:
    """
    Acessa um link e extrai o conte√∫do principal
    
    Args:
        url: URL para acessar
        timeout: Timeout em segundos
        
    Returns:
        Dict com t√≠tulo, descri√ß√£o e conte√∫do, ou None se falhar
    """
    try:
        # Headers para parecer um navegador real
        headers = {
            'User-Agent': (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/91.0.4472.124 Safari/537.36'
            )
        }
        
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        # Parse HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extrair t√≠tulo
        titulo = soup.find('title')
        titulo_texto = titulo.get_text().strip() if titulo else "Sem t√≠tulo"
        
        # Extrair meta descri√ß√£o
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            content = meta_desc.get('content', '')
            descricao = str(content).strip() if content else ""
        else:
            descricao = ""
        
        # Remover scripts e estilos
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # Extrair texto principal
        # Prioriza elementos main, article, ou pega todo o body
        main_content = soup.find('main') or soup.find('article') or soup.find('body')
        
        if main_content:
            # Pega par√°grafos
            paragrafos = main_content.find_all('p')
            conteudo = '\n\n'.join(
                [p.get_text().strip() for p in paragrafos if p.get_text().strip()]
            )
            
            # Limita tamanho (primeiros 3000 caracteres)
            if len(conteudo) > 3000:
                conteudo = conteudo[:3000] + "..."
        else:
            conteudo = "N√£o foi poss√≠vel extrair o conte√∫do principal."
        
        return {
            'url': url,
            'titulo': titulo_texto,
            'descricao': descricao,
            'conteudo': conteudo,
            'sucesso': True
        }
        
    except requests.exceptions.Timeout:
        return {
            'url': url,
            'erro': 'Timeout ao acessar o link',
            'sucesso': False
        }
    except requests.exceptions.RequestException as e:
        return {
            'url': url,
            'erro': f'Erro ao acessar: {str(e)}',
            'sucesso': False
        }
    except Exception as e:
        return {
            'url': url,
            'erro': f'Erro inesperado: {str(e)}',
            'sucesso': False
        }


# ---------------------- FILTRO DE RELEV√ÇNCIA ---------------------- #

# Stopwords bem simples PT/EN, para sobrar s√≥ o que importa na consulta
_STOPWORDS = {
    # PT ‚Äì palavras muito gen√©ricas
    "o", "a", "os", "as", "um", "uma",
    "de", "da", "do", "das", "dos",
    "em", "no", "na", "nos", "nas",
    "por", "para", "pra", "com",
    "sobre", "que", "e", "ou", "se",
    "√©", "foi", "ser",
    "atualizacoes", "atualiza√ß√£o", "atualiza√ß√µes", "atualizacao",
    "informacoes", "informa√ß√£o", "informa√ß√µes",
    "noticias", "not√≠cia", "not√≠cias",
    "recentes", "novas", "ultimas", "√∫ltimas",

    # Termos de busca gen√©ricos que N√ÉO devem influenciar relev√¢ncia
    "busque", "buscar", "busca", "buscas",
    "pesquise", "pesquisa", "pesquisas", "pesquisando",
    "procurar", "procure", "procurando",
    "link", "links",
    "web", "internet", "online",
    "me", "mostre", "mostrar",

    # EN
    "the", "of", "in", "on", "for",
    "and", "or", "is", "are", "to",
    "about", "with", "latest", "news",
    "update", "updates",
}

def _tokenizar_consulta(query: str) -> List[str]:
    """
    Quebra a consulta em tokens relevantes (sem stopwords).
    Ex.: 'Busque atualiza√ß√µes sobre o 3I Atlas' -> ['3i', 'atlas']
    """
    tokens = re.findall(r'\w+', query.lower())
    tokens_filtrados = [t for t in tokens if t not in _STOPWORDS and len(t) > 1]
    return tokens_filtrados or tokens  # se tudo virar stopword, volta tokens brutos


def _pontuar_resultado(resultado: Dict[str, str], tokens: List[str]) -> int:
    """
    D√° uma pontua√ß√£o de relev√¢ncia a um resultado com base na presen√ßa
    dos tokens da consulta no t√≠tulo/snippet.
    """
    titulo = resultado.get('title', '') or resultado.get('titulo', '')
    corpo = resultado.get('body', '') or resultado.get('snippet', '')
    texto_alvo = (titulo + " " + corpo).lower()
    score = 0
    for t in tokens:
        if t in texto_alvo:
            score += 1
    return score


def _dominio_irrelevante(link: str) -> bool:
    """
    Filtra dom√≠nios claramente irrelevantes para a nossa busca,
    incluindo dicion√°rios e conjugadores que estavam aparecendo
    quando a inten√ß√£o era buscar conte√∫do t√©cnico/cient√≠fico.

    Exemplos:
    - Dicion√°rios e conjugadores (quando a consulta √© sobre algoritmos, f√≠sica etc.).
    - Sites gen√©ricos de sin√¥nimos/gram√°tica.
    - Zhihu (resultados em chin√™s fora de contexto).
    """
    try:
        dominio = urlparse(link).netloc.lower()
    except Exception:
        return False

    # Lista simples de dom√≠nios a evitar
    blacklist = [
        "zhihu.com",
        # Dicion√°rios / conjuga√ß√£o / sin√¥nimos que apareceram no seu teste
        "conjugacao.com.br",
        "sinonimos.com.br",
        "infopedia.pt",
        "glosbe.com",
        "dicionarioinformal.com.br",
    ]
    return any(bad in dominio for bad in blacklist)

def buscar_web(query: str, num_resultados: int = 3) -> Optional[List[Dict[str, str]]]:
    """
    Busca na web usando DuckDuckGo, com filtro de relev√¢ncia.

    L√≥gica nova:
    - Extrai as palavras importantes da consulta (sem stopwords).
    - Busca na web.
    - Mant√©m s√≥ resultados cujo t√≠tulo/snippet contenham pelo menos
      um desses tokens relevantes.
    - Descarta dom√≠nios obviamente fora de contexto (ex.: zhihu).
    - Se nada passar no filtro ‚Üí retorna None (sem inventar link lixo).
    """
    try:
        # Tentar usar o novo pacote 'ddgs' primeiro
        try:
            from ddgs import DDGS
        except ImportError:
            # Fallback para o pacote antigo
            from duckduckgo_search import DDGS
        
        tokens = _tokenizar_consulta(query)
        
        ddgs = DDGS()
        resultados_raw = list(ddgs.text(query, max_results=num_resultados * 3))
        
        if not resultados_raw:
            return None

        resultados_filtrados: List[Dict[str, str]] = []

        for r in resultados_raw:
            titulo = r.get('title', '') or ''
            link = r.get('href', '') or ''
            snippet = r.get('body', '') or ''

            if not link:
                continue

            # Filtrar dom√≠nios irrelevantes
            if _dominio_irrelevante(link):
                continue

            score = _pontuar_resultado(r, tokens)

            # Se nenhum token relevante aparece no t√≠tulo/snippet,
            # provavelmente √© resultado gen√©rico ou fora de contexto
            if score <= 0:
                continue

            resultados_filtrados.append({
                'titulo': titulo,
                'link': link,
                'snippet': snippet
            })

        if not resultados_filtrados:
            # Melhor assumir "n√£o achei nada √∫til" do que inventar coisa errada
            return None

        # Limitar ao n√∫mero pedido, j√° com filtro
        return resultados_filtrados[:num_resultados]
        
    except ImportError:
        print("‚ö†Ô∏è Biblioteca duckduckgo-search n√£o instalada. Instale com: pip install duckduckgo-search")
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao buscar na web: {e}")
        return None


def processar_urls_no_texto(texto: str) -> Optional[str]:
    """
    Processa URLs encontradas no texto e retorna um resumo do conte√∫do
    
    Args:
        texto: Texto que pode conter URLs
        
    Returns:
        String com resumo dos conte√∫dos acessados, ou None se n√£o houver URLs
    """
    urls = _extrair_urls(texto)
    
    if not urls:
        return None
    
    resumos = []
    for url in urls:
        resultado = acessar_link(url)
        
        if resultado and resultado.get('sucesso'):
            resumo = f"""
üìÑ **{resultado['titulo']}**
üîó Link: {resultado['url']}

{resultado.get('descricao', '')}

**Conte√∫do:**
{resultado['conteudo'][:500]}...
"""
            resumos.append(resumo)
        else:
            resumos.append(f"‚ùå N√£o foi poss√≠vel acessar: {url}")
    
    conteudo_final = "\n\n---\n\n".join(resumos) if resumos else None
    
    # Adicionar instru√ß√£o para mencionar os links na resposta
    if conteudo_final:
        conteudo_final += (
            "\n\n**INSTRU√á√ÉO**: Ao responder sobre este(s) conte√∫do(s), "
            "SEMPRE mencione o(s) link(s) de origem na sua resposta.\n"
        )
    
    return conteudo_final


def modo_web_ativo() -> bool:
    """Verifica se o modo web est√° ativo via vari√°vel de ambiente"""
    return os.getenv("SOFIA_MODO_WEB", "0") == "1"


def deve_buscar_web(texto: str) -> bool:
    """
    Determina se deve fazer uma busca web baseado no texto
    
    Crit√©rios:
    - Perguntas sobre eventos atuais
    - Solicita√ß√µes expl√≠citas de busca
    - Perguntas sobre informa√ß√µes factuais recentes
    """
    # Palavras-chave que indicam necessidade de busca
    keywords_busca = [
        # Comandos diretos de busca
        'busque', 'pesquise', 'procure', 'encontre',
        'procure na internet', 'procure na web',
        'busca online', 'buscas online', 'modo web', 'buscador',
        'buscar sobre', 'pesquisa sobre',
        
        # Pedidos de links/fontes
        'me d√™ links', 'me d√™ link', 'me passe links', 'me mande links',
        'compartilhe links', 'envie links', 'manda links', 'links sobre',
        'sites sobre', 'p√°ginas sobre', 'fontes sobre',
        
        # Not√≠cias e atualidades
        'o que aconteceu', 'not√≠cias sobre', '√∫ltima novidade',
        'atualiza√ß√µes sobre', 'atualizacoes sobre', 'novidades sobre',
        '√∫ltimas not√≠cias', 'not√≠cias recentes', 'acontecimentos',
        
        # Informa√ß√µes atuais
        'informa√ß√µes sobre', 'informacoes sobre',
        'mais recentes', 'mais atuais', 'atual sobre',
        'pre√ßo atual', 'cota√ß√£o', 'valor atual',
        
        # Eventos espec√≠ficos
        'hoje', 'ontem', 'esta semana', 'este m√™s', 'este ano',
        '2025', '2024',  # Anos recentes
    ]
    
    texto_lower = texto.lower()
    return any(keyword in texto_lower for keyword in keywords_busca)
