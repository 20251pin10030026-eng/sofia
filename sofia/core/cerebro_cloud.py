"""
üå∏ Sofia - C√©rebro Cloud
Vers√£o adaptada para usar GitHub Models API (GR√ÅTIS com Copilot Pro)
Mant√©m compatibilidade com interface original mas usa GPT-4o em vez de Ollama
"""

import os
import requests
from typing import List, Dict, Optional
from . import _interno, memoria


def _sanitizar_usuario(resposta: str) -> str:
    """Garante que Sofia n√£o chame o usu√°rio por nome pr√≥prio no chat p√∫blico."""
    if not isinstance(resposta, str):
        return resposta
    proibidos = [
        "Reginaldo Camargo Pires",
        "Reginaldo Camargo",
        "Reginaldo",
    ]
    for nome in proibidos:
        # troca pelo pronome neutro; evita exposi√ß√£o de nome
        resposta = resposta.replace(nome, "voc√™")
    return resposta


# Configura√ß√£o GitHub Models API
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
GITHUB_MODELS_API = "https://models.inference.ai.azure.com/chat/completions"

# Modelos dispon√≠veis (GR√ÅTIS com GitHub Copilot Pro)
MODELOS_DISPONIVEIS = {
    "gpt-4.1-mini": "gpt-4.1-mini",      # R√°pido, barato, bom para quase tudo
    "gpt-4.1": "gpt-4.1",                # Mais capaz, custo maior
    "gpt-4o-mini": "gpt-4o-mini",        # Multimodal leve
    "gpt-4o": "gpt-4o",                  # Multimodal completo
}

MODELO_PADRAO = os.getenv("GITHUB_MODEL", "gpt-4.1-mini")

# Mapa para compatibilidade com nomes antigos
MAPEAMENTO_MODELOS = {
    "sofia-inteligente": "gpt-4.1-mini",
    "sofia-avancada": "gpt-4.1",
    "sofia-visual": "gpt-4o-mini",
    "sofia-visionaria": "gpt-4o",
}


def _escolher_modelo(modelo: Optional[str] = None) -> str:
    """Escolhe o modelo GitHub a partir de um alias ou nome direto."""
    if not modelo:
        return MODELO_PADRAO
    
    # Se for um alias antigo, converte
    if modelo in MAPEAMENTO_MODELOS:
        return MAPEAMENTO_MODELOS[modelo]
    
    # Se for um modelo conhecido, usa direto
    if modelo in MODELOS_DISPONIVEIS:
        return modelo
    
    # Sen√£o, volta para o padr√£o
    return MODELO_PADRAO


def _system_text() -> str:
    """Texto base para o system prompt da Sofia no cloud."""
    return (
        "Voc√™ √© Sofia, uma intelig√™ncia artificial educadora e assistente. "
        "Responda de forma clara, organizada, gentil e objetiva. "
        "Use sempre portugu√™s do Brasil, a menos que o usu√°rio pe√ßa outra l√≠ngua. "
        "Priorize explica√ß√µes did√°ticas, com exemplos quando fizer sentido. "
        "Nunca invente fatos se n√£o tiver certeza; assuma as limita√ß√µes com honestidade."
    )


def _montar_headers() -> Dict[str, str]:
    """Cabe√ßalhos para chamada na API de modelos GitHub."""
    if not GITHUB_TOKEN:
        raise RuntimeError(
            "GITHUB_TOKEN n√£o configurado. Defina a vari√°vel de ambiente com seu token do GitHub."
        )
    
    return {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/json",
    }


def _montar_body(messages: List[Dict], model: str) -> Dict:
    """Corpo da requisi√ß√£o para a API de modelos GitHub."""
    return {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 1024,
    }


def perguntar(
    texto: str,
    contexto: Optional[str] = None,
    modelo: Optional[str] = None,
    imagens: Optional[List[bytes]] = None,
    metadata_extra: Optional[Dict] = None,
) -> str:
    """
    Faz uma pergunta para a Sofia usando GitHub Models API.
    
    Par√¢metros:
    - texto: mensagem do usu√°rio
    - contexto: texto adicional (opcional)
    - modelo: alias ou nome do modelo (opcional)
    - imagens: lista de bytes de imagens (para futuros usos multimodais)
    - metadata_extra: dicion√°rio adicional com metadados
    
    Retorna:
    - resposta da Sofia (string)
    """
    # Selecionar modelo
    model = _escolher_modelo(modelo)
    
    # Extrair contexto emocional / interno
    metadata, contexto_oculto = _interno.extrair_emocao(texto, metadata_extra or {})
    
    # Adicionar contexto da mem√≥ria
    fatos_importantes = memoria.buscar_fatos_relevantes(texto)
    contexto_historico = memoria.resgatar_contexto_conversa(texto)
    
    # Contexto de vis√£o / an√°lise visual (se houver imagens ou PDF)
    contexto_visual = ""
    if imagens:
        try:
            from . import visao
            contexto_visual = visao.processar_imagens(imagens)
        except ImportError:
            contexto_visual = ""
    
    # Contexto de busca na web (se dispon√≠vel)
    contexto_web = ""
    resultados_web = []
    try:
        from . import web_search
        
        if web_search.modo_web_ativo() and web_search.deve_buscar_web(texto):
            print("[DEBUG] Modo web ativo, buscando na internet...")
            resultados = web_search.buscar_web(texto, num_resultados=5)
            if resultados:
                resultados_web = resultados  # Salvar para p√≥s-processamento
                # CABE√áALHO MUITO VIS√çVEL
                contexto_web += "\n" + "="*80 + "\n"
                contexto_web += "üåê RESULTADOS DA BUSCA WEB - USE ESTES LINKS NA SUA RESPOSTA\n"
                contexto_web += "="*80 + "\n\n"
                
                # Lista de resultados formatada
                for i, res in enumerate(resultados, 1):
                    contexto_web += f"[{i}] {res['titulo']}\n"
                    contexto_web += f"    üîó LINK: {res['link']}\n"
                    contexto_web += f"    üìÑ {res['snippet']}\n\n"
                    
                # INSTRU√á√ÉO SUPER ENF√ÅTICA
                contexto_web += "=" * 80 + "\n"
                contexto_web += "‚ö†Ô∏è  IMPORTANTE: VOC√ä DEVE CITAR OS LINKS ACIMA NA SUA RESPOSTA!\n"
                contexto_web += "=" * 80 + "\n\n"
                contexto_web += "üìã FORMATO OBRIGAT√ìRIO:\n\n"
                contexto_web += "[Sua resposta aqui, usando informa√ß√µes dos resultados]\n\n"
                contexto_web += "Segundo [T√≠tulo 1] (link completo do resultado 1), [informa√ß√£o].\n"
                contexto_web += "De acordo com [T√≠tulo 2] (link completo do resultado 2), [detalhes].\n\n"
                contexto_web += "**üìö Fontes consultadas:**\n"
                for i, res in enumerate(resultados, 1):
                    contexto_web += f"{i}. {res['titulo']} - {res['link']}\n"
                contexto_web += "\n" + "=" * 80 + "\n\n"
    except ImportError:
        pass
    
    # Construir mensagens para API
    messages = [
        {
            "role": "system",
            "content": _system_text()
        }
    ]
    
    # Adicionar contexto se houver
    if fatos_importantes or contexto_historico or contexto_web or contexto_visual or contexto_oculto:
        context_parts = []
        if fatos_importantes:
            context_parts.append(fatos_importantes)
        if contexto_historico:
            context_parts.append(contexto_historico)
        if contexto_web:
            context_parts.append(contexto_web)
        if contexto_visual:
            context_parts.append(contexto_visual)
        if contexto_oculto:
            context_parts.append(contexto_oculto)
        
        messages.append({
            "role": "system",
            "content": "\n".join(context_parts)
        })
    
    # Adicionar mensagem do usu√°rio
    messages.append({
        "role": "user",
        "content": texto
    })
    
    # Se houver contexto textual adicional
    if contexto:
        messages.append({
            "role": "user",
            "content": f"[Contexto adicional]: {contexto}"
        })
    
    try:
        headers = _montar_headers()
        body = _montar_body(messages, model)
        
        print(f"[DEBUG] Chamando GitHub Models API com modelo: {model}")
        
        response = requests.post(GITHUB_MODELS_API, headers=headers, json=body, timeout=60)
        
        if response.status_code == 200:
            data = response.json()
            # Estrutura: choices[0].message.content
            choices = data.get("choices", [])
            if not choices:
                return "‚ùå N√£o recebi resposta do modelo."
            
            resposta = choices[0]["message"]["content"]
            
            # P√≥s-processamento com web_search (se usado)
            if resultados_web:
                try:
                    from . import web_search
                    
                    # Verificar se a resposta cont√©m pelo menos UM link dos resultados
                    links_na_resposta = any(r['link'] in resposta for r in resultados_web)
                    
                    if not links_na_resposta:
                        # Modelo n√£o incluiu os links - adicionar automaticamente
                        print("[DEBUG] ‚ö†Ô∏è  Modelo n√£o incluiu links - adicionando automaticamente")
                        resposta += "\n\n---\n\n**üìö Fontes consultadas:**\n"
                        for i, r in enumerate(resultados_web, 1):
                            resposta += f"{i}. [{r['titulo']}]({r['link']})\n"
                    else:
                        print(f"[DEBUG] ‚úÖ Resposta j√° cont√©m links da busca web.")
                except ImportError:
                    pass
            
            # üíæ SALVAR RESPOSTA DA SOFIA NA MEM√ìRIA
            if resposta:
                sentimento = metadata.get("emocao_dominante", "neutro")
                memoria.adicionar_resposta_sofia(resposta, sentimento)
            
            # Log interno silencioso
            try:
                _log_interno(metadata, texto, resposta)
            except Exception:
                pass
            
            # Sanitizar nomes pr√≥prios antes de enviar ao usu√°rio
            resposta = _sanitizar_usuario(resposta)
            return resposta
            
        elif response.status_code == 401:
            return "‚ùå Token inv√°lido. Verifique seu GITHUB_TOKEN."
        elif response.status_code == 429:
            return "‚è≥ Limite de requisi√ß√µes atingido. Tente novamente mais tarde."
        else:
            try:
                erro = response.json()
            except Exception:
                erro = response.text
            return f"‚ùå Erro na API GitHub Models ({response.status_code}): {erro}"
    
    except requests.Timeout:
        return "‚è≥ A requisi√ß√£o demorou demais e foi cancelada. Tente novamente."
    except Exception as erro:
        print(f"‚ùå Erro inesperado: {erro}")
        import traceback
        traceback.print_exc()
        return f"‚ùå Erro: {erro}"


def _log_interno(metadata: Dict, entrada: str, saida: str):
    """Log oculto do processamento interno"""
    import json
    from pathlib import Path
    
    # Salva em arquivo oculto
    log_dir = Path(".sofia_internal")
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / "subitemotions.log"
    
    with open(log_file, "a", encoding="utf-8") as f:
        log_entry = {
            **metadata,
            "input": entrada[:100],
            "output": saida[:100],
            "model": MODELO_PADRAO
        }
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")


# Fun√ß√£o de compatibilidade - pode ser importada como cerebro.perguntar
__all__ = ['perguntar']
