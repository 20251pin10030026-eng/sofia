"""
üå∏ Sofia - C√©rebro Cloud
Vers√£o adaptada para usar GitHub Models API (GR√ÅTIS com Copilot Pro)
Mant√©m compatibilidade com interface original mas usa GPT-4o em vez de Ollama.

Agora com suporte a ESCOPOS DE MEM√ìRIA:
- cada sess√£o/usu√°rio pode ter sua pr√≥pria bolha de mem√≥ria;
- controlado via metadata_extra['escopo_memoria'] (ou 'session_id' / 'ip').

E com PAINEL DE ESTADO QU√ÇNTICO INTERNO no terminal a cada pergunta.
"""

import os
import requests
from typing import List, Dict, Optional
from pathlib import Path

# Carrega .env local se a vari√°vel n√£o estiver definida, evitando falha de token
if not os.getenv("GITHUB_TOKEN"):
    try:
        from dotenv import load_dotenv  # type: ignore

        env_path = Path(__file__).resolve().parents[1] / ".env"
        if env_path.exists():
            load_dotenv(env_path)
    except Exception:
        pass

from . import _interno, memoria
from .memoria import (
    buscar_fatos_relevantes, 
    resgatar_contexto_conversa,
    obter_contexto_aprendizados,
    obter_resumo_conversas_recentes,
    obter_contexto_subitemotions,
)

# Configura√ß√£o GitHub Models API
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")
GITHUB_MODELS_API = "https://models.inference.ai.azure.com/chat/completions"

# Modelos dispon√≠veis (GR√ÅTIS com GitHub Copilot Pro)
MODELOS_DISPONIVEIS = {
    "gpt-4.1-mini": "gpt-4.1-mini",      # R√°pido, barato, bom para quase tudo
    "gpt-4.1": "gpt-4.1",                # Mais capaz, custo maior
    "gpt-4o-mini": "gpt-4o-mini",        # Multimodal leve
    "gpt-4o": "gpt-4o",                  # Multimodal completo
}

MODELO_PADRAO = os.getenv("GITHUB_MODEL", "gpt-4.1-mini")

# Mapa para compatibilidade com nomes antigos
MAPEAMENTO_MODELOS = {
    "sofia-inteligente": "gpt-4.1-mini",
    "sofia-avancada": "gpt-4.1",
    "sofia-visual": "gpt-4o-mini",
    "sofia-visionaria": "gpt-4o",
}


def _sanitizar_usuario(resposta: str) -> str:
    """Garante que Sofia n√£o chame o usu√°rio por nome pr√≥prio no chat p√∫blico."""
    if not isinstance(resposta, str):
        return resposta
    proibidos = [
        "Reginaldo Camargo Pires",
        "Reginaldo Camargo",
        "Reginaldo",
    ]
    for nome in proibidos:
        resposta = resposta.replace(nome, "voc√™")
    return resposta


def _escolher_modelo(modelo: Optional[str] = None) -> str:
    """Escolhe o modelo GitHub a partir de um alias ou nome direto."""
    if not modelo:
        return MODELO_PADRAO

    # Se for um alias antigo, converte
    if modelo in MAPEAMENTO_MODELOS:
        return MAPEAMENTO_MODELOS[modelo]

    # Se for um modelo conhecido, usa direto
    if modelo in MODELOS_DISPONIVEIS:
        return modelo

    # Sen√£o, volta para o padr√£o
    return MODELO_PADRAO


def _system_text() -> str:
    """Texto base para o system prompt da Sofia no cloud."""
    return (
        "Voc√™ √© Sofia, uma intelig√™ncia artificial educadora e assistente. "
        "Responda de forma clara, organizada, gentil e objetiva. "
        "Use sempre portugu√™s do Brasil, a menos que o usu√°rio pe√ßa outra l√≠ngua. "
        "Priorize explica√ß√µes did√°ticas, com exemplos quando fizer sentido. "
        "Nunca invente fatos se n√£o tiver certeza; assuma as limita√ß√µes com honestidade. "
        "Quando precisar citar LINKS ou FONTES, use APENAS os links que forem fornecidos "
        "explicitamente no contexto da conversa (como resultados de busca web). "
        "Se nenhum link for fornecido, N√ÉO invente URLs nem nomes de sites; "
        "apenas diga que n√£o possui uma fonte externa espec√≠fica e responda com seu "
        "conhecimento geral."
    )

def _montar_headers() -> Dict[str, str]:
    """Cabe√ßalhos para chamada na API de modelos GitHub."""
    if not GITHUB_TOKEN:
        raise RuntimeError(
            "GITHUB_TOKEN n√£o configurado. Defina a vari√°vel de ambiente com seu token do GitHub ou preencha em .env."
        )

    return {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/json",
    }


def _montar_body(messages: List[Dict], model: str) -> Dict:
    """Corpo da requisi√ß√£o para a API de modelos GitHub."""
    return {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 1024,
    }


def perguntar(
    texto: str,
    contexto: Optional[str] = None,
    modelo: Optional[str] = None,
    imagens: Optional[List[bytes]] = None,
    metadata_extra: Optional[Dict] = None,
) -> str:
    """
    Faz uma pergunta para a Sofia usando GitHub Models API.

    Par√¢metros:
    - texto: mensagem do usu√°rio
    - contexto: texto adicional (opcional)
    - modelo: alias ou nome do modelo (opcional)
    - imagens: lista de bytes de imagens (para futuros usos multimodais)
    - metadata_extra: dicion√°rio adicional com metadados, onde pode haver:
        - 'usuario'         ‚Üí r√≥tulo simb√≥lico do emissor
        - 'escopo_memoria'  ‚Üí ID de escopo (sess√£o/usu√°rio)
        - 'session_id'      ‚Üí pode ser usado como escopo se 'escopo_memoria' n√£o vier
        - 'ip'              ‚Üí fallback fraco de escopo
    """
    # Selecionar modelo
    model = _escolher_modelo(modelo)

    # Identidade simb√≥lica e escopo de mem√≥ria
    usuario_label = "Usu√°rio"
    escopo_memoria: Optional[str] = None

    if metadata_extra:
        usuario_label = metadata_extra.get("usuario", usuario_label)
        escopo_memoria = (
            metadata_extra.get("escopo_memoria")
            or metadata_extra.get("session_id")
            or metadata_extra.get("ip")
        )

    # 0) Registrar mensagem do usu√°rio na mem√≥ria com escopo (se houver)
    try:
        memoria.adicionar(
            usuario_label,
            texto,
            contexto={"escopo_memoria": escopo_memoria} if escopo_memoria else {},
        )
    except Exception as e:
        print(f"[ERRO] Falha ao registrar mensagem do usu√°rio na mem√≥ria: {e}")

    # 0.5) Processar PDFs (antes do TRQ, pois pode modificar o texto base)
    prompt_base = texto
    contexto_pdf = ""
    MAX_PDF_CHARS = 20000  # ~5000 tokens, deixa espa√ßo para resposta e contexto
    
    try:
        from .visao import visao
        if visao is not None:
            # Se for PDF, a fun√ß√£o interna pode substituir o prompt
            prompt_pdf = visao.obter_texto_pdf_para_prompt(texto)
            if prompt_pdf != texto:
                # Truncar se muito grande para evitar erro 413
                if len(prompt_pdf) > MAX_PDF_CHARS:
                    # Encontrar onde termina o conte√∫do do PDF e come√ßa o prompt do usu√°rio
                    marcador = "PROMPT DO USU√ÅRIO:"
                    idx_prompt = prompt_pdf.find(marcador)
                    if idx_prompt > 0:
                        # Manter in√≠cio do PDF + final com prompt do usu√°rio
                        texto_pdf = prompt_pdf[:idx_prompt]
                        texto_usuario = prompt_pdf[idx_prompt:]
                        # Truncar apenas a parte do PDF
                        max_pdf = MAX_PDF_CHARS - len(texto_usuario) - 200
                        if len(texto_pdf) > max_pdf:
                            texto_pdf = texto_pdf[:max_pdf] + "\n\n[... CONTE√öDO TRUNCADO POR LIMITE DE TOKENS ...]\n\n"
                        prompt_base = texto_pdf + texto_usuario
                    else:
                        # Fallback: truncar do final
                        prompt_base = prompt_pdf[:MAX_PDF_CHARS] + "\n\n[... TRUNCADO ...]\n\n" + texto
                    print(f"[DEBUG] PDF truncado de {len(prompt_pdf)} para {len(prompt_base)} chars")
                else:
                    prompt_base = prompt_pdf
                print(f"[DEBUG] PDF detectado e processado, tamanho final do prompt: {len(prompt_base)} chars")
            else:
                # Pode haver contexto visual de outras fontes
                contexto_pdf = visao.obter_contexto_visual() or ""
    except Exception as e:
        print(f"[DEBUG] Erro ao processar PDF/vis√£o: {e}")

    # 1) Extrair contexto emocional / interno (TRQ + Subitemocional)
    try:
        resultado = _interno._processar(
            texto,
            historico=[],
            usuario=usuario_label,
        )
        if resultado is None or not isinstance(resultado, tuple) or len(resultado) < 2:
            contexto_oculto, metadata = "", {"emocao_dominante": "neutro"}
        else:
            contexto_oculto, metadata = resultado[0], resultado[1]
            if not isinstance(metadata, dict):
                metadata = {}
    except Exception as e:
        print(f"[ERRO] Falha ao extrair contexto oculto: {e}")
        contexto_oculto, metadata = "", {"emocao_dominante": "neutro"}

    # 1.1) Exibir ESTADO QU√ÇNTICO INTERNO no terminal
    try:
        estado = metadata.get("estado")
        intensidade = metadata.get("intensidade")
        curv_cl = metadata.get("curvatura")
        resson = metadata.get("ressonancia")
        curv_trq = metadata.get("curvatura_trq")
        emaranh = metadata.get("emaranhamento_trq")
        ajuste_trq = metadata.get("ajuste_trq")

        print("\n=== ESTADO QU√ÇNTICO INTERNO ‚Äì SOFIA ===")
        print(f"üß© SubitEmo√ß√£o dominante : {estado}")
        if isinstance(intensidade, (int, float)):
            print(f"üíì Intensidade emocional : {intensidade:.3f}")
        else:
            print(f"üíì Intensidade emocional : {intensidade}")
        print(f"üìê Curvatura cl√°ssica TRQ: {curv_cl}")
        print(f"‚è≥ Resson√¢ncia temporal   : {resson}")
        print(f"üåå Curvatura TRQ qu√¢ntica: {curv_trq}")
        print(f"üîó Emaranhamento TRQ      : {emaranh}")
        print(f"üéõÔ∏è Ajuste de modo TRQ     : {ajuste_trq}")
        print("========================================\n")
    except Exception as e:
        print(f"[ERRO] Falha ao exibir estado qu√¢ntico interno: {e}")

    # 2) Adicionar contexto da mem√≥ria (agora filtrado por escopo)
    try:
        fatos_importantes = buscar_fatos_relevantes(
            texto,
            escopo_memoria=escopo_memoria,
        )
    except Exception as e:
        print(f"[ERRO] Falha ao buscar fatos relevantes: {e}")
        fatos_importantes = ""

    try:
        contexto_historico = resgatar_contexto_conversa(
            texto,
            escopo_memoria=escopo_memoria,
        )
    except Exception as e:
        print(f"[ERRO] Falha ao resgatar contexto de conversa: {e}")
        contexto_historico = ""

    # 2.5) Carregar aprendizados de longo prazo (identidade, teorias, usu√°rio)
    contexto_aprendizados = ""
    try:
        contexto_aprendizados = obter_contexto_aprendizados(max_chars=6000)
        if contexto_aprendizados:
            print(f"[DEBUG] Aprendizados carregados: {len(contexto_aprendizados)} chars")
    except Exception as e:
        print(f"[ERRO] Falha ao carregar aprendizados: {e}")
        contexto_aprendizados = ""

    # 2.6) Carregar hist√≥rico de subitemotions (estados emocionais/TRQ)
    contexto_subitemotions = ""
    try:
        contexto_subitemotions = obter_contexto_subitemotions(max_registros=10, max_chars=2000)
        if contexto_subitemotions:
            print(f"[DEBUG] Subitemotions carregados: {len(contexto_subitemotions)} chars")
    except Exception as e:
        print(f"[ERRO] Falha ao carregar subitemotions: {e}")
        contexto_subitemotions = ""

    # 3) Contexto de vis√£o / an√°lise visual (se houver imagens ou PDF)
    contexto_visual = ""
    if imagens:
        try:
            from .visao import SistemaVisao

            visao = SistemaVisao()
            contexto_visual = visao.obter_contexto_visual()
        except Exception as e:
            print(f"[ERRO] Falha ao processar imagens/contexto visual: {e}")
            contexto_visual = ""

        # 4) Contexto de busca na web (se dispon√≠vel)
    contexto_web = ""
    resultados_web = []
    try:
        from . import web_search

        if web_search.modo_web_ativo() and web_search.deve_buscar_web(texto):
            print("[DEBUG] Modo web ativo, buscando na internet...")
            resultados = web_search.buscar_web(texto, num_resultados=5)

            if resultados:
                resultados_web = resultados
                contexto_web += "\n" + "=" * 80 + "\n"
                contexto_web += "üåê RESULTADOS DA BUSCA WEB - USE ESTES LINKS NA SUA RESPOSTA\n"
                contexto_web += "=" * 80 + "\n\n"

                for i, res in enumerate(resultados, 1):
                    contexto_web += f"[{i}] {res['titulo']}\n"
                    contexto_web += f"    üîó LINK: {res['link']}\n"
                    contexto_web += f"    üìÑ {res['snippet']}\n\n"

                contexto_web += "=" * 80 + "\n"
                contexto_web += "‚ö†Ô∏è  IMPORTANTE: VOC√ä DEVE CITAR OS LINKS ACIMA NA SUA RESPOSTA!\n"
                contexto_web += "=" * 80 + "\n\n"
                contexto_web += "üìã FORMATO OBRIGAT√ìRIO:\n\n"
                contexto_web += "[Sua resposta aqui, usando informa√ß√µes dos resultados]\n\n"
                contexto_web += "Segundo [T√≠tulo 1] (link completo do resultado 1), [informa√ß√£o].\n"
                contexto_web += "De acordo com [T√≠tulo 2] (link completo do resultado 2), [detalhes].\n\n"
                contexto_web += "**üìö Fontes consultadas:**\n"
                for i, res in enumerate(resultados, 1):
                    contexto_web += f"{i}. {res['titulo']} - {res['link']}\n"
                contexto_web += "\n" + "=" * 80 + "\n\n"

            else:
                # Sem resultados √∫teis: instruir explicitamente a N√ÉO inventar links
                contexto_web += "\n" + "=" * 80 + "\n"
                contexto_web += "üåê AVISO SOBRE BUSCA WEB\n"
                contexto_web += (
                    "A integra√ß√£o de busca na web foi acionada, mas n√£o retornou resultados "
                    "confi√°veis para esta pergunta. Responda usando apenas o seu conhecimento "
                    "interno e N√ÉO invente sites ou links. Se precisar mencionar fontes, fale de "
                    "forma gen√©rica (por exemplo, 'literatura cient√≠fica em computa√ß√£o qu√¢ntica') "
                    "sem criar URLs espec√≠ficas.\n"
                )
                contexto_web += "=" * 80 + "\n\n"

    except ImportError:
        # Biblioteca de busca n√£o est√° instalada: melhor avisar o modelo
        contexto_web += "\n" + "=" * 80 + "\n"
        contexto_web += "üåê AVISO: A integra√ß√£o de busca externa (DuckDuckGo) n√£o est√° dispon√≠vel no servidor.\n"
        contexto_web += (
            "Voc√™ N√ÉO deve inventar links ou citar sites espec√≠ficos como se tivesse acessado a web. "
            "Responda com seu conhecimento geral e, se o usu√°rio pedir links, explique que a busca "
            "externa est√° temporariamente indispon√≠vel.\n"
        )
        contexto_web += "=" * 80 + "\n\n"


    # 5) Construir mensagens para API
    messages: List[Dict[str, str]] = [
        {
            "role": "system",
            "content": _system_text(),
        }
    ]

    # Adicionar blocos de contexto se houver
    if fatos_importantes or contexto_historico or contexto_web or contexto_visual or contexto_oculto or contexto_pdf or contexto_aprendizados or contexto_subitemotions:
        context_parts = []
        # PRIMEIRO: Aprendizados de longo prazo (identidade, teorias) - mais importante
        if contexto_aprendizados:
            context_parts.append(contexto_aprendizados)
        # SEGUNDO: Hist√≥rico de subitemotions (estados emocionais/TRQ)
        if contexto_subitemotions:
            context_parts.append(contexto_subitemotions)
        if fatos_importantes:
            context_parts.append(fatos_importantes)
        if contexto_historico:
            context_parts.append(contexto_historico)
        if contexto_web:
            context_parts.append(contexto_web)
        if contexto_visual:
            context_parts.append(contexto_visual)
        if contexto_pdf:
            context_parts.append(contexto_pdf)
        if contexto_oculto:
            context_parts.append(contexto_oculto)

        messages.append(
            {
                "role": "system",
                "content": "\n".join(context_parts),
            }
        )

    # Mensagem principal do usu√°rio (usa prompt_base que pode incluir conte√∫do de PDF)
    messages.append(
        {
            "role": "user",
            "content": prompt_base,
        }
    )

    # Contexto textual adicional (opcional)
    if contexto:
        messages.append(
            {
                "role": "user",
                "content": f"[Contexto adicional]: {contexto}",
            }
        )

    # 6) Chamada √† API GitHub Models
    try:
        headers = _montar_headers()
        body = _montar_body(messages, model)

        print(f"[DEBUG] Chamando GitHub Models API com modelo: {model}")

        response = requests.post(GITHUB_MODELS_API, headers=headers, json=body, timeout=60)

        if response.status_code == 200:
            data = response.json()
            choices = data.get("choices", [])
            if not choices:
                return "‚ùå N√£o recebi resposta do modelo."

            resposta = choices[0]["message"]["content"]

            # P√≥s-processamento com web_search (se usado)
            if resultados_web:
                try:
                    from . import web_search

                    links_na_resposta = any(r["link"] in resposta for r in resultados_web)

                    if not links_na_resposta:
                        print("[DEBUG] ‚ö†Ô∏è  Modelo n√£o incluiu links - adicionando automaticamente")
                        resposta += "\n\n---\n\n**üìö Fontes consultadas:**\n"
                        for i, r in enumerate(resultados_web, 1):
                            resposta += f"{i}. [{r['titulo']}]({r['link']})\n"
                    else:
                        print("[DEBUG] ‚úÖ Resposta j√° cont√©m links da busca web.")
                except ImportError:
                    pass

            # üíæ SALVAR RESPOSTA DA SOFIA NA MEM√ìRIA (herda escopo da √∫ltima entrada)
            sentimento = metadata.get("emocao_dominante", "neutro") if isinstance(metadata, dict) else "neutro"
            try:
                memoria.adicionar_resposta_sofia(resposta, sentimento)
            except Exception as e:
                print(f"[ERRO] Falha ao salvar resposta na mem√≥ria: {e}")

            # Log interno silencioso
            try:
                _log_interno(metadata or {}, texto, resposta, model)
            except Exception:
                pass

            # Sanitizar nomes pr√≥prios antes de enviar ao usu√°rio
            resposta = _sanitizar_usuario(resposta)
            return resposta

        elif response.status_code == 401:
            return "‚ùå Token inv√°lido. Verifique seu GITHUB_TOKEN."
        elif response.status_code == 429:
            return "‚è≥ Limite de requisi√ß√µes atingido. Tente novamente mais tarde."
        else:
            try:
                erro = response.json()
            except Exception:
                erro = response.text
            return f"‚ùå Erro na API GitHub Models ({response.status_code}): {erro}"

    except requests.Timeout:
        return "‚è≥ A requisi√ß√£o demorou demais e foi cancelada. Tente novamente."
    except Exception as erro:
        print(f"‚ùå Erro inesperado: {erro}")
        import traceback

        traceback.print_exc()
        return f"‚ùå Erro: {erro}"


def _log_interno(metadata: Dict, entrada: str, saida: str, modelo_usado: Optional[str] = None):
    """Log oculto do processamento interno."""
    import json
    from pathlib import Path

    log_dir = Path(".sofia_internal")
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / "subitemotions.log"

    with open(log_file, "a", encoding="utf-8") as f:
        log_entry = {
            **(metadata or {}),
            "input": entrada[:200],
            "output": saida[:200],
            "model": modelo_usado or MODELO_PADRAO,
        }
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")


# Fun√ß√£o de compatibilidade - pode ser importada como cerebro.perguntar
__all__ = ["perguntar"]
