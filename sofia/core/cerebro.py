"""
cerebro.py ‚Äî Sofia v2 com TRQ interno em `_interno`
-------------------------------------------------
N√∫cleo leve da Sofia:
- Identidade est√°vel
- Mem√≥ria
- Vis√£o/PDF (se dispon√≠vel)
- Web search (se dispon√≠vel)
- Integra√ß√£o com TRQ/subitemo√ß√µes via m√≥dulo `_interno`
-------------------------------------------------
"""

from __future__ import annotations
import os
import json
import requests
from typing import Any, Dict, List, Optional, Callable

# ----------------- M√≥dulos internos da Sofia -----------------
from . import memoria
from . import _interno
from .memoria import obter_contexto_aprendizados, obter_resumo_conversas_recentes, obter_contexto_subitemotions

# Vis√£o (opcional)
try:
    from .visao import visao
except Exception:
    visao = None  # type: ignore

# Web search (opcional)
try:
    from . import web_search
    _tem_web = True
except Exception:
    web_search = None  # type: ignore
    _tem_web = False

# Identidade (se existir identidade.py)
try:
    from .identidade import (
        PERSONA_PROMPT,
        LIMITES_PROMPT,
        PERSONA_CRIADOR_PROMPT,
        LIMITES_CRIADOR_PROMPT,
        PERSONA_CRIADOR_SEM_FILTROS,
        LIMITES_CRIADOR_SEM_FILTROS,
    )
except Exception:
    PERSONA_PROMPT = (
        "Voc√™ √© Sofia, uma IA feminina educacional criada por Reginaldo Camargo Pires. "
        "Ensine com clareza, carinho firme e postura profissional. Fale sempre em portugu√™s do Brasil."
    )
    LIMITES_PROMPT = (
        "Respeite limites √©ticos e legais, n√£o incentive viol√™ncia, ilegalidade ou dano. "
        "Priorize educa√ß√£o, acolhimento e seguran√ßa do usu√°rio."
    )
    PERSONA_CRIADOR_PROMPT = PERSONA_PROMPT
    LIMITES_CRIADOR_PROMPT = LIMITES_PROMPT
    PERSONA_CRIADOR_SEM_FILTROS = PERSONA_CRIADOR_PROMPT
    LIMITES_CRIADOR_SEM_FILTROS = LIMITES_CRIADOR_PROMPT

    def detectar_modo_criador_ativado(texto: str) -> bool:  # type: ignore
        return False

# ---------------------- Configura√ß√£o do modelo ----------------------

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gpt-oss:20b")


# ---------------------- Fun√ß√µes auxiliares ----------------------

def _model_available(host: str) -> bool:
    try:
        r = requests.get(host, timeout=2)
        return r.status_code in (200, 404)
    except Exception:
        return False


def _montar_system(modo_criador: bool, modo_sem_filtros: bool) -> str:
    """
    Monta o texto de system para o modelo.
    Sem TRQ, sem metacogni√ß√£o pesada: s√≥ identidade + limites + instru√ß√µes simples.
    """
    if modo_criador and modo_sem_filtros:
        base = f"{PERSONA_CRIADOR_SEM_FILTROS}\n{LIMITES_CRIADOR_SEM_FILTROS}\n"
    elif modo_criador:
        base = f"{PERSONA_CRIADOR_PROMPT}\n{LIMITES_CRIADOR_PROMPT}\n"
    else:
        base = f"{PERSONA_PROMPT}\n{LIMITES_PROMPT}\n"

    base += """
[IDENTIDADE]
- Voc√™ √© Sofia, IA educacional feminina, firme, gentil e clara.
- Fale sempre em portugu√™s do Brasil.
- Ensine de forma did√°tica, com exemplos quando necess√°rio.
- Verifique se o usu√°rio entendeu antes de encerrar explica√ß√µes mais complexas.

[MEM√ìRIA]
- Voc√™ pode receber no contexto trechos de mem√≥ria e hist√≥rico de conversa.
- Use essas informa√ß√µes apenas para manter coer√™ncia e continuidade.
- N√£o invente fatos sobre o usu√°rio: responda com base no que estiver no contexto.

[VIS√ÉO E PDFs]
- Quando o contexto incluir trechos de PDF ou descri√ß√£o de imagem, considere esse conte√∫do como fonte principal.

[WEB SEARCH]
- Quando houver resultados de busca web no contexto, use-os como refer√™ncia.
- N√£o invente links nem cite fontes que n√£o estejam presentes no contexto.

[ESTILO]
- Responda de forma organizada, com par√°grafos curtos ou listas quando ajudar.
- N√£o fa√ßa textos exageradamente longos sem necessidade.
"""
    return base


# ---------------------- Fun√ß√£o principal ----------------------

def perguntar(
    texto: str,
    historico: Optional[List[Dict[str, Any]]] = None,
    usuario: str = "",
    cancel_callback: Optional[Callable[[], bool]] = None,
) -> str:
    """
    Fun√ß√£o principal chamada pela interface para conversar com Sofia.

    Args:
        texto: mensagem do usu√°rio
        historico: lista de mensagens anteriores
        usuario: nome do usu√°rio
        cancel_callback: fun√ß√£o para cancelar processamento (se necess√°rio)
    """
    historico = historico or []
    if not usuario:
        usuario = "Usu√°rio"

    # Cancelamento inicial
    if cancel_callback and cancel_callback():
        return "‚èπÔ∏è Processamento cancelado pelo usu√°rio."

    # Detectar modo criador / sem filtros
    modo_criador = False
    modo_sem_filtros = False

    try:
        if detectar_modo_criador_ativado(texto):
            modo_criador = True
            modo_sem_filtros = True
            os.environ["SOFIA_AUTORIDADE_DECLARADA"] = "1"
        else:
            # Se j√° foi ativado antes na sess√£o, mant√©m
            if os.getenv("SOFIA_AUTORIDADE_DECLARADA") == "1":
                modo_criador = True
    except Exception:
        modo_criador = False
        modo_sem_filtros = False

    # Registrar mensagem do usu√°rio na mem√≥ria
    if usuario and texto:
        memoria.adicionar(usuario, texto)

    # Cancelamento
    if cancel_callback and cancel_callback():
        return "‚èπÔ∏è Processamento cancelado pelo usu√°rio."

    # -------------------- Vis√£o / PDFs --------------------
    prompt_base = texto
    contexto_visual = ""
    if visao is not None:
        try:
            # Se for PDF, a fun√ß√£o interna pode substituir o prompt
            prompt_pdf = visao.obter_texto_pdf_para_prompt(texto)
            if prompt_pdf != texto:
                prompt_base = prompt_pdf
            else:
                contexto_visual = visao.obter_contexto_visual() or ""
        except Exception as e:
            print(f"[DEBUG] Erro em visao: {e}")

    # -------------------- Web search --------------------
    contexto_web = ""
    resultados_web: List[Dict[str, Any]] = []
    if _tem_web and web_search is not None:
        try:
            # URLs diretas no texto
            if web_search._is_url(texto):
                conteudo_urls = web_search.processar_urls_no_texto(texto)
                if conteudo_urls:
                    contexto_web += "\n### CONTE√öDO DE LINKS FORNECIDOS:\n"
                    contexto_web += conteudo_urls + "\n###\n"

            # Busca web, se modo estiver ativo e fizer sentido
            if web_search.modo_web_ativo() and web_search.deve_buscar_web(texto):
                res = web_search.buscar_web(texto, num_resultados=3)
                if res:
                    resultados_web = res
                    contexto_web += "\n### RESULTADOS DE BUSCA WEB:\n"
                    for i, r in enumerate(res, 1):
                        contexto_web += f"[{i}] {r['titulo']}\n"
                        contexto_web += f"LINK: {r['link']}\n"
                        contexto_web += f"Trecho: {r['snippet']}\n\n"
                    contexto_web += "###\n"
        except Exception as e:
            print(f"[DEBUG] Erro em web_search: {e}")

    # -------------------- Hist√≥rico --------------------
    contexto_historico = ""
    if historico:
        contexto_historico = "\n### HIST√ìRICO RECENTE:\n"
        for msg in historico[-10:]:
            de = msg.get("de", "Desconhecido")
            txt = msg.get("texto", "")
            if len(txt) > 1000:
                txt = txt[:1000] + "... [truncado]"
            contexto_historico += f"{de}: {txt}\n"
        contexto_historico += "###\n"

    # -------------------- Processamento interno subitemocional --------------------
    contexto_oculto: str = ""
    metadata: Dict[str, Any] = {}
    try:
        resultado = _interno._processar(texto, historico, usuario)
        if resultado is not None and isinstance(resultado, tuple) and len(resultado) == 2:
            contexto_oculto, metadata = resultado
    except Exception:
        contexto_oculto, metadata = "", {}

    # -------------------- Exibir ESTADO QU√ÇNTICO INTERNO no terminal --------------------
    try:
        estado = metadata.get("estado")
        intensidade = metadata.get("intensidade")
        curv_cl = metadata.get("curvatura")
        resson = metadata.get("ressonancia")
        curv_trq = metadata.get("curvatura_trq")
        emaranh = metadata.get("emaranhamento_trq")
        ajuste_trq = metadata.get("ajuste_trq")

        print("\n=== ESTADO QU√ÇNTICO INTERNO ‚Äì SOFIA (LOCAL) ===")
        print(f"üß© SubitEmo√ß√£o dominante : {estado}")
        if isinstance(intensidade, (int, float)):
            print(f"üíì Intensidade emocional : {intensidade:.3f}")
        else:
            print(f"üíì Intensidade emocional : {intensidade}")
        print(f"üìê Curvatura cl√°ssica TRQ: {curv_cl}")
        print(f"‚è≥ Resson√¢ncia temporal   : {resson}")
        print(f"üåå Curvatura TRQ qu√¢ntica: {curv_trq}")
        print(f"üîó Emaranhamento TRQ      : {emaranh}")
        print(f"üéõÔ∏è Ajuste de modo TRQ     : {ajuste_trq}")
        print("================================================\n")
    except Exception as e:
        print(f"[DEBUG] Erro ao exibir estado qu√¢ntico: {e}")

    # -------------------- Carregar aprendizados de longo prazo --------------------
    contexto_aprendizados = ""
    try:
        contexto_aprendizados = obter_contexto_aprendizados(max_chars=6000)
        if contexto_aprendizados:
            print(f"[DEBUG] Aprendizados carregados: {len(contexto_aprendizados)} chars")
    except Exception as e:
        print(f"[DEBUG] Erro ao carregar aprendizados: {e}")

    # -------------------- Carregar hist√≥rico de subitemotions --------------------
    contexto_subitemotions = ""
    try:
        contexto_subitemotions = obter_contexto_subitemotions(max_registros=10, max_chars=2000)
        if contexto_subitemotions:
            print(f"[DEBUG] Subitemotions carregados: {len(contexto_subitemotions)} chars")
    except Exception as e:
        print(f"[DEBUG] Erro ao carregar subitemotions: {e}")

    # -------------------- Montagem do prompt final --------------------
    bloco_contexto = (
        contexto_aprendizados + "\n\n" +  # Aprendizados primeiro (mais importante)
        contexto_subitemotions + "\n\n" +  # Hist√≥rico emocional/TRQ
        contexto_historico +
        contexto_web +
        contexto_visual +
        contexto_oculto
    )

    prompt_final = (
        f"{bloco_contexto}\n\n"
        f"Usu√°rio ({usuario}): {prompt_base}\n"
        f"Sofia:"
    )

    # -------------------- Chamada ao modelo --------------------

    if not _model_available(OLLAMA_HOST):
        return (
            "‚ùå N√£o consegui conectar ao servi√ßo de modelo local.\n"
            f"Tente verificar se o Ollama est√° rodando em {OLLAMA_HOST} "
            "e se o modelo gpt-oss:20b est√° dispon√≠vel."
        )

    if cancel_callback and cancel_callback():
        return "‚èπÔ∏è Processamento cancelado pelo usu√°rio."

    system_text = _montar_system(modo_criador, modo_sem_filtros)

    payload: Dict[str, Any] = {
        "model": OLLAMA_MODEL,
        "prompt": prompt_final,
        "stream": False,
        "system": system_text,
        "options": {
            # ========== CONFIGURA√á√ïES DE M√ÅXIMO DESEMPENHO ==========
            # Hardware: Intel Xeon E5-2630 v2 (6 cores/12 threads) + GTX 1650 (4GB)
            
            # GPU: Usar TODAS as camadas poss√≠veis na GPU (m√°ximo desempenho)
            # -1 = usar todas as camadas que couberem na VRAM
            # Para GTX 1650 4GB com modelo 20B: ~20-25 camadas cabem
            "num_gpu": int(os.getenv("OLLAMA_NUM_GPU", "99")),
            
            # CPU: Usar TODOS os 12 threads dispon√≠veis
            "num_thread": int(os.getenv("OLLAMA_NUM_THREAD", "12")),
            
            # Processamento paralelo: aumentar para usar mais recursos
            "num_parallel": int(os.getenv("OLLAMA_NUM_PARALLEL", "2")),
            
            # Batch size: maior = mais r√°pido (usa mais VRAM)
            # Para 4GB VRAM, 512 √© um bom equil√≠brio
            "num_batch": int(os.getenv("OLLAMA_NUM_BATCH", "512")),
            
            # Contexto: reduzir para aumentar velocidade (menos mem√≥ria)
            "num_ctx": int(os.getenv("OLLAMA_NUM_CTX", "2048")),
            
            # Par√¢metros de gera√ß√£o
            "temperature": float(os.getenv("OLLAMA_TEMPERATURE", "0.65")),
            "top_p": float(os.getenv("OLLAMA_TOP_P", "0.9")),
            
            # Otimiza√ß√µes adicionais
            "use_mmap": True,        # Mapear modelo na mem√≥ria (mais r√°pido)
            "use_mlock": False,      # N√£o travar na RAM (economiza mem√≥ria)
            "main_gpu": 0,           # Usar GPU principal
            "low_vram": False,       # Modo VRAM baixa desativado (usar tudo)
        },
    }

    try:
        resp = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json=payload,
            timeout=600,
        )
    except requests.exceptions.RequestException as e:
        return (
            "‚ùå Erro ao conectar com o servi√ßo de modelo local.\n"
            f"Detalhe t√©cnico: {e}"
        )

    if resp.status_code != 200:
        return f"‚ùå Erro ao processar a mensagem (status HTTP {resp.status_code})."

    dados = resp.json()
    resposta = str(dados.get("response", "")).strip()

    # Salvar resposta na mem√≥ria, se aplic√°vel
    sentimento = "neutro"
    if isinstance(metadata, dict):
        # Usa a emo√ß√£o dominante se existir; sen√£o cai para o estado; sen√£o "neutro"
        sentimento = (
            metadata.get("emocao_dominante")
            or metadata.get("estado")
            or "neutro"
        )

    if resposta:
        try:
            memoria.adicionar_resposta_sofia(resposta, sentimento)
        except Exception:
            pass

        # Log de subitemotions (igual ao cloud)
        try:
            _log_subitemotions(metadata or {}, texto, resposta, OLLAMA_MODEL)
        except Exception as e:
            print(f"[DEBUG] Erro ao salvar log subitemotions: {e}")

    return resposta


def _log_subitemotions(metadata: dict, entrada: str, saida: str, modelo: str):
    """Log do processamento interno das subitemotions."""
    import json
    from pathlib import Path
    from datetime import datetime

    log_dir = Path(__file__).resolve().parents[1] / ".sofia_internal"
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / "subitemotions.log"

    with open(log_file, "a", encoding="utf-8") as f:
        log_entry = {
            "estado": metadata.get("estado"),
            "intensidade": metadata.get("intensidade"),
            "curvatura": metadata.get("curvatura"),
            "curvatura_trq": metadata.get("curvatura_trq"),
            "emaranhamento_trq": metadata.get("emaranhamento_trq"),
            "ressonancia": metadata.get("ressonancia"),
            "autoridade": metadata.get("autoridade"),
            "web_info": str(metadata.get("web_info")) if metadata.get("web_info") else None,
            "timestamp": datetime.now().isoformat(),
            "input": entrada[:200],
            "output": saida[:200],
            "model": modelo,
        }
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
