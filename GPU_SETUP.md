# ðŸŽ® ConfiguraÃ§Ã£o de GPU para Sofia

## VisÃ£o Geral

A Sofia agora estÃ¡ configurada para usar **aceleraÃ§Ã£o por GPU** (NVIDIA GeForce GTX 1650) junto com a CPU para gerar respostas mais rÃ¡pidas e eficientes.

## ðŸš€ ConfiguraÃ§Ã£o RÃ¡pida

### 1. Verificar GPU DisponÃ­vel

```powershell
# Ver GPU no Gerenciador de Tarefas
# Pressione: Ctrl + Shift + Esc
# VÃ¡ em: Desempenho > GPU

# Ou use o script Python
python verificar_gpu.py
```

### 2. Aplicar ConfiguraÃ§Ãµes de GPU

```powershell
# Execute o script de configuraÃ§Ã£o
.\setup_gpu.ps1

# Ou defina manualmente:
$env:OLLAMA_GPU_LAYERS = "999"
$env:OLLAMA_NUM_PARALLEL = "4"
```

### 3. Reiniciar Ollama

```powershell
# Parar Ollama (se estiver rodando)
Stop-Process -Name "ollama" -Force -ErrorAction SilentlyContinue

# Iniciar novamente
ollama serve
```

### 4. Verificar se GPU estÃ¡ sendo usada

```powershell
python verificar_gpu.py
```

## âš™ï¸ ConfiguraÃ§Ãµes Aplicadas

A Sofia foi configurada com:

```json
{
  "num_gpu": 999,        // Usa TODAS as camadas na GPU
  "num_thread": 8,       // 8 threads da CPU para paralelismo
  "num_ctx": 4096,       // Contexto de 4K tokens
  "temperature": 0.7,    // Criatividade moderada
  "top_p": 0.9          // Diversidade nas respostas
}
```

### LocalizaÃ§Ã£o no CÃ³digo

Arquivo: `sofia/core/cerebro.py` (linhas ~220-236)

## ðŸ“Š Performance Esperada

### Com CPU apenas:
- Velocidade: ~5-15 tokens/segundo
- Tempo de resposta: Lento (30-60s para respostas longas)
- Uso de memÃ³ria: Alto

### Com GPU GTX 1650:
- Velocidade: ~30-60 tokens/segundo
- Tempo de resposta: RÃ¡pido (5-15s para respostas longas)
- Uso de memÃ³ria: Otimizado (compartilhado GPU/CPU)

### Monitoramento

**Gerenciador de Tarefas do Windows:**
1. Abra com `Ctrl + Shift + Esc`
2. VÃ¡ em "Desempenho"
3. Selecione "GPU"
4. Durante uso da Sofia, vocÃª deve ver:
   - **GPU 3D**: 10-80% (processamento)
   - **Copy**: ~14% (transferÃªncia de dados)
   - **MemÃ³ria dedicada**: 3.4-4.0 GB usados
   - **Temperatura**: ~48Â°C (normal)

## ðŸ”§ SoluÃ§Ã£o de Problemas

### GPU nÃ£o estÃ¡ sendo usada?

**1. Verificar drivers NVIDIA:**
```powershell
nvidia-smi
```
Se nÃ£o funcionar, atualize os drivers em: https://www.nvidia.com/download/index.aspx

**2. Reinstalar Ollama com suporte CUDA:**
```powershell
# Desinstalar versÃ£o atual
ollama uninstall

# Baixar e instalar versÃ£o com CUDA
# https://ollama.ai/download
```

**3. Verificar variÃ¡veis de ambiente:**
```powershell
Get-ChildItem Env: | Where-Object { $_.Name -like "*OLLAMA*" }
```

Deve mostrar:
- `OLLAMA_GPU_LAYERS = 999`
- `OLLAMA_NUM_PARALLEL = 4`

**4. Testar modelo especÃ­fico:**
```bash
ollama run mistral "teste de gpu"
```

### Performance ainda lenta?

**1. Usar modelo quantizado:**
```bash
# Modelos menores = mais rÃ¡pidos
ollama pull mistral:7b-q4_0  # 4-bit quantizado
ollama pull mistral:7b-q5_0  # 5-bit (mais preciso)
```

**2. Ajustar contexto:**
Reduza `num_ctx` se tiver pouca VRAM:
```python
"num_ctx": 2048  # Em vez de 4096
```

**3. Limitar GPU layers:**
Se VRAM insuficiente:
```python
"num_gpu": 32  # Em vez de 999
```

## ðŸ“ˆ ComparaÃ§Ã£o de Modelos

| Modelo | Tamanho | VRAM NecessÃ¡ria | Velocidade (GTX 1650) |
|--------|---------|-----------------|----------------------|
| mistral:7b | ~4.1 GB | ~4 GB | â­â­â­â­ (rÃ¡pido) |
| mistral:7b-q4 | ~3.8 GB | ~3.5 GB | â­â­â­â­â­ (muito rÃ¡pido) |
| llama2:7b | ~3.8 GB | ~3.5 GB | â­â­â­â­ (rÃ¡pido) |
| llama2:13b | ~7.4 GB | ~8 GB | â­ (lento/overflow) |

**RecomendaÃ§Ã£o para GTX 1650 (4GB):** 
- âœ… `mistral:7b-q4_0` (melhor balance)
- âœ… `mistral:7b` (qualidade mÃ¡xima)
- âŒ Modelos 13b+ (VRAM insuficiente)

## ðŸŽ¯ OtimizaÃ§Ãµes AvanÃ§adas

### 1. PrÃ©-carregar modelo na GPU
```bash
# MantÃ©m modelo carregado
ollama run mistral ""
# Deixe rodando em background
```

### 2. Ajustar prioridade do processo
```powershell
# No Gerenciador de Tarefas
# Detalhes > ollama.exe > BotÃ£o direito > Definir prioridade > Alta
```

### 3. Configurar afinidade de CPU
```powershell
# Usar cores especÃ­ficos (0-7)
$process = Get-Process ollama
$process.ProcessorAffinity = 0xFF  # Todos os 8 cores
```

## ðŸ“ Scripts Ãšteis

### verificar_gpu.py
Testa se GPU estÃ¡ sendo usada:
```bash
python verificar_gpu.py
```

### setup_gpu.ps1
Aplica configuraÃ§Ãµes otimizadas:
```bash
.\setup_gpu.ps1
```

### reiniciar_sofia.ps1
Reinicia servidor com configuraÃ§Ãµes:
```bash
.\reiniciar_sofia.ps1
```

## ðŸ” Logs e Debug

### Ver logs do Ollama:
```powershell
# Habilitar debug
$env:OLLAMA_DEBUG = "1"
ollama serve
```

### Monitorar uso de GPU em tempo real:
```powershell
# Atualiza a cada 1 segundo
while ($true) { 
    Clear-Host
    nvidia-smi
    Start-Sleep -Seconds 1
}
```

## ðŸ“š ReferÃªncias

- [Ollama GPU Documentation](https://github.com/ollama/ollama/blob/main/docs/gpu.md)
- [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)
- [Ollama API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)

## âœ… Checklist de VerificaÃ§Ã£o

- [ ] Drivers NVIDIA atualizados
- [ ] Ollama instalado com suporte CUDA
- [ ] VariÃ¡veis de ambiente configuradas (`setup_gpu.ps1`)
- [ ] Modelo compatÃ­vel baixado (`mistral:7b-q4_0`)
- [ ] GPU aparece no Gerenciador de Tarefas
- [ ] Script `verificar_gpu.py` mostra >30 tokens/s
- [ ] Sofia responde mais rÃ¡pido que antes

---

**Atualizado em**: 7 de novembro de 2025
**Hardware**: NVIDIA GeForce GTX 1650 (4GB VRAM)
**Status**: âœ… Configurado e otimizado
